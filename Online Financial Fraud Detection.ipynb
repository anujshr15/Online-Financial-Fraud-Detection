{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of fraudulent data instances: 0.1727485630620034\n",
      "percentage of normal data instances: 99.827251436938\n",
      "(227845, 30) (227845,)\n",
      "0    227451\n",
      "1       394\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "data.head()\n",
    "\n",
    "data['Class'].value_counts()\n",
    "\n",
    "len(data['Class'])\n",
    "\n",
    "print(\"percentage of fraudulent data instances: {}\".format(data['Class'].value_counts()[1] *100 /len(data['Class'])))\n",
    "print(\"percentage of normal data instances: {}\".format(data['Class'].value_counts()[0] *100 /len(data['Class'])))\n",
    "\n",
    "# Rescaling the data\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "rs = RobustScaler()\n",
    "\n",
    "data['scaled_amount'] = rs.fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "data['scaled_time'] = rs.fit_transform(data['Time'].values.reshape(-1, 1))\n",
    "\n",
    "data.drop(['Amount', 'Time'], axis = 1, inplace = True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = data.drop(['Class'], axis = 1)\n",
    "y = data['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0, \n",
    "                                                    stratify = y)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(y_train.value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results=[]\n",
    "all_dataset=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " def training_testing_function(X,y,name):\n",
    "        corr = X.corr()\n",
    "        columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "        for i in range(corr.shape[0]):\n",
    "            for j in range(i+1, corr.shape[0]):\n",
    "                if corr.iloc[i,j] >= 0.9:\n",
    "                    if columns[j]:\n",
    "                        columns[j] = False\n",
    "        selected_columns = X.columns[columns]\n",
    "        X = X[selected_columns]\n",
    "\n",
    "\n",
    "        rfc = RandomForestClassifier();\n",
    "\n",
    "        # fit random forest classifier on the training set\n",
    "        rfc.fit(X, y.values.ravel());\n",
    "        # extract important features\n",
    "        score = np.round(rfc.feature_importances_,3)\n",
    "\n",
    "        importances = pd.DataFrame({'feature':X.columns,'importance':score})\n",
    "        importances = importances.sort_values('importance',ascending=False).set_index('feature')\n",
    "\n",
    "        final_features=list(importances[:20].index)\n",
    "\n",
    "        X=X[final_features]\n",
    "\n",
    "        # Train KNeighborsClassifier Model\n",
    "        KNN_Classifier = KNeighborsClassifier(n_jobs=-1)\n",
    "        # KNN_Classifier.fit(X_train_random_oversampled, y_train_random_oversampled.values.ravel()); \n",
    "\n",
    "        # Train LogisticRegression Model\n",
    "        LGR_Classifier = LogisticRegression(multi_class='auto', random_state=1,solver='lbfgs',max_iter=400)\n",
    "        # LGR_Classifier.fit(X_train_random_oversampled, y_train_random_oversampled.values.ravel());\n",
    "\n",
    "        # Train Gaussian Naive Baye Model\n",
    "        GNB_Classifier = GaussianNB()\n",
    "        # GNB_Classifier.fit(X_train_random_oversampled, y_train_random_oversampled.values.ravel())\n",
    "\n",
    "        # Train Decision Tree Model\n",
    "        DTC_Classifier = tree.DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "        # DTC_Classifier.fit(X_train_random_oversampled, y_train_random_oversampled.values.ravel());\n",
    "\n",
    "        # Train RandomForestClassifier Model\n",
    "        #RF_Classifier = RandomForestClassifier(criterion='entropy', n_jobs=-1, random_state=0)\n",
    "        #RF_Classifier.fit(X_train, Y_train);  \n",
    "\n",
    "        # Train SVM Model\n",
    "        #SVC_Classifier = SVC(random_state=0)\n",
    "        #SVC_Classifier.fit(X_train, Y_train)\n",
    "\n",
    "        ## Train Ensemble Model (This method combines all the individual models above except RandomForest)\n",
    "        combined_model = [('Naive Baye Classifier', GNB_Classifier), \n",
    "                         ('Decision Tree Classifier', DTC_Classifier), \n",
    "                         ('KNeighborsClassifier', KNN_Classifier), \n",
    "                         ('LogisticRegression', LGR_Classifier)\n",
    "                        ]\n",
    "        VC =  VotingClassifier(estimators = combined_model,voting = 'soft')\n",
    "        VC.fit(X, y.values.ravel());\n",
    "\n",
    "        models = []\n",
    "        #models.append(('SVM Classifier', SVC_Classifier))\n",
    "        # models.append(('Naive Baye Classifier', GNB_Classifier))\n",
    "        # models.append(('Decision Tree Classifier', DTC_Classifier))\n",
    "        #models.append(('RandomForest Classifier', RF_Classifier))\n",
    "        # models.append(('KNeighborsClassifier', KNN_Classifier))\n",
    "        # models.append(('LogisticRegression', LGR_Classifier))\n",
    "        models.append(('VotingClassifier', VC))\n",
    "\n",
    "        for i, v in models:\n",
    "    #         accuracy = metrics.accuracy_score(y_test.values.ravel(), v.predict(X_test[final_features]))\n",
    "    #         confusion_matrix = metrics.confusion_matrix(y_test.values.ravel(), v.predict(X_test[final_features]))\n",
    "    #         classification = metrics.classification_report(y_test.values.ravel(), v.predict(X_test[final_features]))\n",
    "            f1 = metrics.f1_score(y_test.values.ravel(),v.predict(X_test[final_features]))\n",
    "            all_results.append((name,f1))\n",
    "            print(\"{} Completed\".format(name))\n",
    "    #         print('============================== {} Model Test Results =============================='.format(i))\n",
    "    #         print()\n",
    "    #         print (\"Model Accuracy:\" \"\\n\", accuracy)\n",
    "    #         print()\n",
    "    #         print(\"Confusion matrix:\" \"\\n\", confusion_matrix)\n",
    "    #         print()\n",
    "    #         print(\"Classification report:\" \"\\n\", classification) \n",
    "    #         print()        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Random Undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 394, 1: 394})\n"
     ]
    }
   ],
   "source": [
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "# fit and apply the transform\n",
    "all_dataset[\"X_train_undersample\"], all_dataset[\"y_train_undersample\"] = undersample.fit_resample(X_train, y_train)\n",
    "# summarize class distribution\n",
    "print(Counter(all_dataset[\"y_train_undersample\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_testing_function(all_dataset[\"X_train_undersample\"],all_dataset[\"y_train_undersample\"],\"Random Undersample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Instance Hardness Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import InstanceHardnessThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape Counter({0: 222288, 1: 394})\n"
     ]
    }
   ],
   "source": [
    "iht = InstanceHardnessThreshold(sampling_strategy='majority', random_state=42)\n",
    "all_dataset[\"X_train_res\"], all_dataset[\"y_train_res\"] = iht.fit_resample(X_train, y_train)\n",
    "print('Resampled dataset shape %s' % Counter(all_dataset[\"y_train_res\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_testing_function(all_dataset[\"X_train_res\"],all_dataset[\"y_train_res\"],\"IHT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Cluster Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset[\"X_cluster_centroids\"],all_dataset[\"y_cluster_centroids\"]  = ClusterCentroids().fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster centroids Completed\n"
     ]
    }
   ],
   "source": [
    "training_testing_function(all_dataset[\"X_cluster_centroids\"],all_dataset[\"y_cluster_centroids\"],\"Cluster centroids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SMOTE', 0.7545454545454545),\n",
       " ('ADASYN', 0.6259541984732825),\n",
       " ('Borderline SMOTE', 0.7843137254901962),\n",
       " ('SVM SMOTE', 0.7839195979899498),\n",
       " ('Cluster centroids', 0.3110307414104882)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Near Miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Near Miss Completed\n"
     ]
    }
   ],
   "source": [
    "all_dataset['X_near_miss'],all_dataset['y_near_miss'] = NearMiss().fit_resample(X_train,y_train)\n",
    "training_testing_function(all_dataset['X_near_miss'],all_dataset['y_near_miss'],\"Near Miss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SMOTE', 0.7545454545454545),\n",
       " ('ADASYN', 0.6259541984732825),\n",
       " ('Borderline SMOTE', 0.7843137254901962),\n",
       " ('SVM SMOTE', 0.7839195979899498),\n",
       " ('Cluster centroids', 0.3110307414104882),\n",
       " ('Near Miss', 0.007315406567788095)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. One Sided Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Sided Selection Completed\n"
     ]
    }
   ],
   "source": [
    "all_dataset['X_one_sided_selection'],all_dataset['y_one_sided_selection'] = OneSidedSelection().fit_resample(X_train,y_train)\n",
    "training_testing_function(all_dataset['X_one_sided_selection'],all_dataset['y_one_sided_selection'],\"One Sided Selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SMOTE', 0.7545454545454545),\n",
       " ('ADASYN', 0.6259541984732825),\n",
       " ('Borderline SMOTE', 0.7843137254901962),\n",
       " ('SVM SMOTE', 0.7839195979899498),\n",
       " ('Cluster centroids', 0.3110307414104882),\n",
       " ('Near Miss', 0.007315406567788095),\n",
       " ('One Sided Selection', 0.7789473684210526)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Tomek Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset['X_tomek_links'],all_dataset['y_tomek_links'] = TomekLinks().fit_resample(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomek Links Completed\n"
     ]
    }
   ],
   "source": [
    "training_testing_function(all_dataset['X_tomek_links'],all_dataset['y_tomek_links'],\"Tomek Links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tomek Links', 0.7914438502673796)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 227451), (1, 227451)]\n"
     ]
    }
   ],
   "source": [
    "ros = RandomOverSampler(random_state=0)\n",
    "all_dataset[\"X_train_random_oversampled\"], all_dataset[\"y_train_random_oversampled\"] = ros.fit_resample(X_train, y_train)\n",
    "from collections import Counter\n",
    "print(sorted(Counter(all_dataset[\"y_train_random_oversampled\"]).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_testing_function(all_dataset[\"X_train_random_oversampled\"],all_dataset[\"y_train_random_oversampled\"],\"Random Oversampled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 227451), (1, 227451)]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "all_dataset[\"X_smote\"], all_dataset[\"y_smote\"] = SMOTE().fit_resample(X_train, y_train)\n",
    "print(sorted(Counter(all_dataset[\"y_smote\"]).items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE Completed\n"
     ]
    }
   ],
   "source": [
    "training_testing_function(all_dataset['X_smote'],all_dataset['y_smote'],\"SMOTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SMOTE', 0.7545454545454545)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 227451), (1, 227448)]\n"
     ]
    }
   ],
   "source": [
    "all_dataset['X_adasyn'],all_dataset['y_adasyn'] = ADASYN().fit_resample(X_train, y_train)\n",
    "print(sorted(Counter(all_dataset[\"y_adasyn\"]).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADASYN Completed\n"
     ]
    }
   ],
   "source": [
    "training_testing_function(all_dataset['X_adasyn'],all_dataset['y_adasyn'],\"ADASYN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SMOTE', 0.7545454545454545), ('ADASYN', 0.6259541984732825)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Variations of SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE,SVMSMOTE,KMeansSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Borderline SMOTE Completed\n",
      "SVM SMOTE Completed\n"
     ]
    }
   ],
   "source": [
    "all_dataset[\"X_borderline_smote\"],all_dataset[\"y_borderline_smote\"] = BorderlineSMOTE().fit_resample(X_train,y_train)\n",
    "all_dataset[\"X_svm_smote\"],all_dataset[\"y_svm_smote\"] = SVMSMOTE().fit_resample(X_train,y_train)\n",
    "\n",
    "# X_kmeans_smote,y_kmeans_smote = KMeansSMOTE().fit_resample(X_train,y_train)\n",
    "\n",
    "\n",
    "# all_dataset[\"X_kmeans_smote\"],all_dataset[\"y_kmeans_smote\"]= X_kmeans_smote,y_kmeans_smote\n",
    "\n",
    "training_testing_function(all_dataset[\"X_borderline_smote\"],all_dataset[\"y_borderline_smote\"],\"Borderline SMOTE\")\n",
    "training_testing_function(all_dataset[\"X_svm_smote\"],all_dataset[\"y_svm_smote\"],\"SVM SMOTE\")\n",
    "\n",
    "# training_testing_function(all_dataset[\"X_kmeans_smote\"],all_dataset[\"y_kmeans_smote\"],\"KMeans SMOTE\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SMOTE', 0.7545454545454545),\n",
       " ('ADASYN', 0.6259541984732825),\n",
       " ('Borderline SMOTE', 0.7843137254901962),\n",
       " ('SVM SMOTE', 0.7839195979899498)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of Undersampling and Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE Tomek Completed\n"
     ]
    }
   ],
   "source": [
    "all_dataset['X_smote_tomek'],all_dataset['y_smote_tomek'] = SMOTETomek().fit_resample(X_train,y_train)\n",
    "training_testing_function(all_dataset['X_smote_tomek'],all_dataset['y_smote_tomek'],\"SMOTE Tomek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tomek Links', 0.7914438502673796), ('SMOTE Tomek', 0.7248908296943231)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>isFlaggedFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>9839.64</td>\n",
       "      <td>C1231006815</td>\n",
       "      <td>170136.0</td>\n",
       "      <td>160296.36</td>\n",
       "      <td>M1979787155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>1864.28</td>\n",
       "      <td>C1666544295</td>\n",
       "      <td>21249.0</td>\n",
       "      <td>19384.72</td>\n",
       "      <td>M2044282225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C1305486145</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C553264065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C840083671</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C38997010</td>\n",
       "      <td>21182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>11668.14</td>\n",
       "      <td>C2048537720</td>\n",
       "      <td>41554.0</td>\n",
       "      <td>29885.86</td>\n",
       "      <td>M1230701703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
       "0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n",
       "1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n",
       "2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n",
       "3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n",
       "4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n",
       "\n",
       "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
       "0  M1979787155             0.0             0.0        0               0  \n",
       "1  M2044282225             0.0             0.0        0               0  \n",
       "2   C553264065             0.0             0.0        1               0  \n",
       "3    C38997010         21182.0             0.0        1               0  \n",
       "4  M1230701703             0.0             0.0        0               0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = pd.read_csv(\"dataset2.csv\")\n",
    "dataset2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6362620, 11)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.drop(\"isFlaggedFraud\",axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=dataset2['isFraud']\n",
    "dataset2.drop('isFraud',axis=1,inplace=True)\n",
    "X=dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the destination account balances being zero is a strong indicator of fraud, we do not impute the account balance (before the transaction is made) with a statistic or from a distribution with a subsequent adjustment for the amount transacted. Doing so would mask this indicator of fraud and make fraudulent transactions appear genuine. Instead, below we replace the value of 0 with -1 which will be more useful to a suitable machine-learning (ML) algorithm detecting fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[(X.oldbalanceDest == 0) & (X.newbalanceDest == 0) & (X.amount != 0), ['oldbalanceDest', 'newbalanceDest']] = - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data also has several transactions with zero balances in the originating account both before and after a non-zero amount is transacted. In this case, the fraction of such transactions is much smaller in fraudulent (0.3%) compared to genuine transactions (47%). Once again, from similar reasoning as above, instead of imputing a numerical value we replace the value of 0 with a null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[(X.oldbalanceOrg == 0) & (X.newbalanceOrig == 0) & (X.amount != 0),['oldbalanceOrg', 'newbalanceOrig']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivated by the possibility of zero-balances serving to differentiate between fraudulent and genuine transactions, we create 2 new features (columns) recording errors in the originating and destination accounts for each transaction. These new features turn out to be important in obtaining the best performance from the ML algorithm that we will finally use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['errorbalanceOrg'] = X.newbalanceOrig + X.amount - X.oldbalanceOrg\n",
    "X['errorbalanceDest'] = X.oldbalanceDest + X.amount - X.newbalanceDest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
